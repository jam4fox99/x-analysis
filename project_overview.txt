# X/Twitter Trading Signal Analysis & Optimization Project

## 1. High-Level Project Goal

The primary objective of this project is to create a sophisticated, end-to-end automated trading analysis pipeline. The system leverages social media sentiment from X (formerly Twitter) to identify potential stock trading signals, rigorously backtests these signals against historical market data, and systematically optimizes exit strategies to maximize profitability and win rates. It is designed to transform unstructured social media chatter into actionable, data-driven trading insights.

---

## 2. The Four-Stage Pipeline

The project operates in a four-stage, sequential pipeline: Data Acquisition -> Signal Generation -> Performance Backtesting -> Strategy Optimization.

### Stage 1: Data Acquisition (Tweet Scraping)

This stage is responsible for gathering the raw data (tweets) needed for analysis.

-   **Data Source:** Instead of the expensive official X API, the system uses the **Apify Tweet Scraper API**. This provides a cost-effective and scalable solution for fetching large volumes of tweets.
-   **Fetching Mechanism:** The system is configured to scrape up to 10,000 of the latest tweets from a specified X user (e.g., 'SarwanJohn').
-   **Asynchronous & Robust:** To handle potentially long-running requests for thousands of tweets without timing out, the system implements a robust, asynchronous API call pattern:
    1.  It sends an initial request to *start* the scraping job on Apify's servers.
    2.  It then polls a status endpoint periodically to check if the job is complete.
    3.  Once the job is finished, it makes a final call to retrieve the full dataset of tweets.
-   **Data Caching:** The fetched tweets are saved locally to `tweets.csv`. This acts as a cache, preventing the need for repeated, costly API calls during development and re-analysis.
-   **Filtering:** The system includes a flag (`INCLUDE_RETWEETS`) to control whether retweets should be included in the dataset, allowing for cleaner, more focused analysis on original content if desired.

*Orchestrated by: `main.py`*

### Stage 2: Intelligent Signal Generation (AI Analysis)

This is the core "intelligence" of the system, where raw tweets are converted into structured trading signals.

-   **AI Model:** The system utilizes OpenAI's powerful language models (specifically **`gpt-4o-mini`**) to analyze the text.
-   **Contextual Batching Strategy:** Instead of analyzing tweets one-by-one or in a simple chronological stream, the system employs a "smarter" method. It groups all tweets by the stock ticker they mention (e.g., all `$AAPL` tweets are batched together, all `$TSLA` tweets are batched together). This provides the AI with the complete, ordered narrative for a single stock, allowing it to detect trends, hype cycles, and sentiment shifts with much higher accuracy.
-   **Advanced AI Prompting:** A detailed and carefully engineered prompt guides the AI. It instructs the model to act as a conservative financial analyst and to classify tweets into one of four distinct actions, with strict definitions to avoid ambiguity:
    -   `entry`: Clear "buy" or "accumulate" signals.
    -   `exit`: Clear "sell" or "take profit" signals.
    -   `hold`: Explicit advice to maintain a position.
    -   `update`: Neutral news or observations without trading intent.
-   **Multithreaded Processing:** To accelerate the analysis of potentially hundreds of ticker-specific batches, the AI analysis is parallelized using a `ThreadPoolExecutor`, significantly reducing the total processing time.
-   **Output:** The generated signals (containing timestamp, ticker, action, and original tweet text) are saved to `signals.csv`, which serves as the foundational dataset for all subsequent stages.

*Orchestrated by: `main.py`*

### Stage 3: Dual-Mode Performance Backtesting

This stage simulates historical trading based on the generated signals to evaluate their past performance. It runs two separate simulations to provide both an aggressive and a conservative performance outlook.

-   **Data Sources:** Uses the AI-generated `signals.csv` and fetches historical intraday stock price data from the **Alpha Vantage API**.
-   **Mode 1: "All Entries" (Aggressive Strategy)**
    -   Treats *every* `entry` signal as a new, independent trade.
    -   Uses a First-In, First-Out (FIFO) queue (`collections.deque`) to manage multiple open positions for the same stock. When an `exit` signal appears, it closes the earliest open position.
-   **Mode 2: "First Entry Only" (Conservative Strategy)**
    -   Only trades the *very first* chronological `entry` signal for each unique stock. All subsequent `entry` signals for that same stock are ignored.
    -   This simulates a more cautious approach, preventing over-exposure to a single asset.
-   **ROI Calculation:** For each completed trade, it calculates the Return on Investment (ROI). It also calculates a "holding ROI" for positions that remain open at the end of the simulation, based on the last known stock price.
-   **Output:** The detailed results of each simulation, including every trade, entry/exit prices, and ROI, are saved to separate files: `roi_analysis_all_entries.csv` and `roi_analysis_first_entry_only.csv`.

*Orchestrated by: `backtest.py`*

### Stage 4: Grid Search Optimization for Exit Strategy

This final stage aims to find the optimal exit strategy by testing a vast number of Take-Profit (TP) and Stop-Loss (SL) combinations.

-   **Methodology:** It performs a **grid search**, a systematic method for hyperparameter tuning. It iterates through a predefined grid of parameters:
    -   Take-Profit (TP) levels from 5% to 200% (in 5% increments).
    -   Stop-Loss (SL) levels from 5% to 100% (in 5% increments).
-   **Forward Simulation:** For each `entry` signal and every (TP, SL) pair, it runs a forward-looking simulation. It fetches up to 60 days of daily historical data from Alpha Vantage and checks each day to see if the TP price (`entry_price * (1 + tp_percentage)`) or SL price (`entry_price * (1 - sl_percentage)`) was hit.
-   **Dual Optimization Objectives:** The grid search aims to find the best (TP, SL) combination according to two different metrics:
    1.  **Highest Average ROI:** The combination that yields the greatest average return across all trades.
    2.  **Highest Win Rate:** The combination that results in the highest percentage of winning trades.
-   **Data Caching:** To minimize redundant API calls to Alpha Vantage, it caches the historical price data for each ticker.
-   **Output:** The system saves the detailed trade lists corresponding to the two optimal strategies into `optimal_roi_trades.csv` and `optimal_win_rate_trades.csv`.

*Orchestrated by: `grid_search_optimizer.py`*

---

## 3. Technical Stack & Key Components

-   **Programming Language:** Python 3
-   **Core Libraries:**
    -   `pandas`: For all data manipulation, from reading CSVs to structuring analysis results.
    -   `requests`: For all HTTP API communication with Apify and Alpha Vantage.
    -   `openai`: For interacting with the GPT models.
    -   `python-dotenv`: For securely managing API keys from a `.env` file.
    -   `concurrent.futures`: For enabling multithreading in the AI analysis stage.
    -   `collections.deque`: For implementing the FIFO logic in the backtester.
-   **APIs:**
    -   **Apify:** For scalable and cost-effective tweet scraping.
    -   **OpenAI:** For natural language understanding and signal classification.
    -   **Alpha Vantage:** For historical daily and intraday stock price data.
-   **Environment & Version Control:**
    -   `.env` file: Securely stores all necessary API keys (`APIFY_API_TOKEN`, `OPENAI_API_KEY`, `ALPHA_VANTAGE_API_KEY`).
    -   `.gitignore`: Properly configured to exclude the `.env` file, generated `.csv` data, and Python cache files from version control.
    -   **Git & GitHub:** Used for version control, allowing for iterative development, rollbacks, and code collaboration. 